import streamlit as st
import os
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

from langchain.chains import ConversationChain
from langchain.chains.retrieval_qa.base import RetrievalQA
from langchain.memory import ConversationBufferMemory
from dotenv import load_dotenv

from llm import get_llm
from rag import build_retriever_from_pdf

st.set_page_config(page_title="LLM Assistant", page_icon="ğŸ¤–")
st.title("ğŸ¤– å¤§æ¨¡å‹åŠ©æ‰‹ï¼ˆèŠå¤© + çŸ¥è¯†åº“RAGï¼‰")

# åŠ è½½ .env æ–‡ä»¶
load_dotenv()

# --- Key check ---
api_key = os.getenv("DASHSCOPE_API_KEY")
if not api_key:
    st.error("""
    æœªæ£€æµ‹åˆ° DASHSCOPE_API_KEYã€‚è¯·é€‰æ‹©ä»¥ä¸‹ä»»ä¸€æ–¹å¼é…ç½®ï¼š

    1. åœ¨é¡¹ç›®æ ¹ç›®å½•åˆ›å»º .env æ–‡ä»¶ï¼Œå†…å®¹ä¸ºï¼š
       DASHSCOPE_API_KEY=ä½ çš„é˜¿é‡Œäº‘API_Key

    2. è®¾ç½®ç³»ç»Ÿç¯å¢ƒå˜é‡ï¼š
       Windows: setx DASHSCOPE_API_KEY "ä½ çš„Key"
       Linux/macOS: export DASHSCOPE_API_KEY="ä½ çš„Key"
    """)
    st.stop()

# --- Sidebar: mode switch ---
mode = st.sidebar.radio("é€‰æ‹©æ¨¡å¼", ["èŠå¤©æ¨¡å¼", "çŸ¥è¯†åº“æ¨¡å¼ï¼ˆRAGï¼‰"])
model_name = st.sidebar.selectbox("é€‰æ‹©æ¨¡å‹", ["qwen-turbo", "qwen-plus", "qwen-max"], index=0)
if st.sidebar.button("æ¸…ç©ºå¯¹è¯"):
    st.session_state.messages = []
    # ä¹Ÿé‡ç½®èŠå¤©è®°å¿†
    llm = get_llm(model_name=model_name)
    st.session_state.chat_chain = ConversationChain(llm=llm, memory=ConversationBufferMemory())
system_prompt = st.sidebar.text_area(
    "ç³»ç»Ÿæç¤ºè¯",
    value="ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„åŠ©ç†ã€‚è‹¥æ²¡æœ‰ä¾æ®è¯·æ˜ç¡®è¯´ä¸çŸ¥é“ã€‚"
)
k = st.sidebar.slider("æ£€ç´¢æ¡æ•° TopK", 2, 10, 4)

# --- init session ---
if "messages" not in st.session_state:
    st.session_state.messages = []

if "chat_chain" not in st.session_state:
    llm = get_llm(model_name=model_name)
    st.session_state.chat_chain = ConversationChain(
        llm=llm,
        memory=ConversationBufferMemory(),
    )

# RAG chain is built only when user uploads a PDF
if "rag_chain" not in st.session_state:
    st.session_state.rag_chain = None  # åˆå§‹åŒ–ä¸º Noneï¼Œç­‰å¾…ç”¨æˆ·ä¸Šä¼  PDF

# If user changes model, rebuild chains safely
if "last_model_name" not in st.session_state:
    st.session_state.last_model_name = model_name

if st.session_state.last_model_name != model_name:
    # rebuild chat chain
    llm = get_llm(model_name=model_name)
    st.session_state.chat_chain = ConversationChain(
        llm=llm,
        memory=ConversationBufferMemory(),
    )
    # rebuild rag chain if exists
    if st.session_state.rag_chain is not None:
        # keep retriever but swap llm
        old_retriever = st.session_state.rag_chain.retriever
        st.session_state.rag_chain = RetrievalQA.from_chain_type(
            llm=llm,
            retriever=old_retriever,
            return_source_documents=True
        )
    st.session_state.last_model_name = model_name
    st.session_state.messages = []  # clear UI history on model switch (simple & safe)

# --- RAG setup UI ---
if mode == "çŸ¥è¯†åº“æ¨¡å¼ï¼ˆRAGï¼‰":
    uploaded = st.sidebar.file_uploader("ä¸Šä¼  PDF ä½œä¸ºçŸ¥è¯†åº“", type=["pdf"])
    if uploaded is not None:
        os.makedirs(".cache", exist_ok=True)
        pdf_path = os.path.join(".cache", "kb.pdf")
        with open(pdf_path, "wb") as f:
            f.write(uploaded.read())

        with st.sidebar:
            st.success("PDF å·²ä¸Šä¼ ï¼Œæ­£åœ¨æ„å»ºçŸ¥è¯†åº“ï¼ˆé¦–æ¬¡ä¼šç¨æ…¢ï¼‰...")

        try:
            retriever = build_retriever_from_pdf(pdf_path)
            llm = get_llm(model_name=model_name)
            st.session_state.rag_chain = RetrievalQA.from_chain_type(
                llm=llm,
                retriever=retriever,
                return_source_documents=True
            )
            st.sidebar.success("çŸ¥è¯†åº“æ„å»ºå®Œæˆ âœ…")
        except Exception as e:
            st.sidebar.error(f"æ„å»ºå¤±è´¥ï¼š{e}")
            st.session_state.rag_chain = None

# --- display history ---
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# --- input ---
prompt = st.chat_input("è¯·è¾“å…¥ä½ çš„é—®é¢˜â€¦")
if prompt:
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        with st.spinner("æ€è€ƒä¸­..."):
            try:
                if mode == "èŠå¤©æ¨¡å¼":
                    answer = st.session_state.chat_chain.predict(input=f"[ç³»ç»Ÿ]\n{system_prompt}\n\n[ç”¨æˆ·]\n{prompt}")
                    sources = None
                else:
                    if st.session_state.rag_chain is None:
                        answer = "è¯·å…ˆåœ¨å·¦ä¾§ä¸Šä¼ ä¸€ä¸ª PDFï¼Œæ„å»ºçŸ¥è¯†åº“åå†æé—®ã€‚"
                        sources = None
                    else:
                        result = st.session_state.rag_chain({"query": f"{system_prompt}\n\n{prompt}"})
                        answer = result["result"]
                        sources = result.get("source_documents", [])
            except Exception as e:
                answer = f"å‘ç”Ÿé”™è¯¯ï¼š{e}"
                sources = None

        st.markdown(answer)

        if sources:
            with st.expander("ğŸ“š æŸ¥çœ‹å¼•ç”¨æ¥æº"):
                for i, d in enumerate(sources, 1):
                    meta = d.metadata or {}
                    page = meta.get("page", meta.get("page_number", "æœªçŸ¥"))
                    if isinstance(page, int):
                        page += 1
                    st.markdown(f"**[{i}] é¡µç ï¼š{page}**")
                    st.write(d.page_content[:400] + "â€¦")

    st.session_state.messages.append({"role": "assistant", "content": answer})